{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os\n",
    "from pathlib import Path\n",
    "from time import sleep\n",
    "\n",
    "sys.path.append(\n",
    "    os.path.abspath(\n",
    "        r\"S:\\Data\\Stores Payroll\\FY21\\99_Master Scripts (DO NOT EDIT)\\dB_Connector\"))\n",
    "\n",
    "from connector import *\n",
    "\n",
    "sys.path.append(\n",
    "    os.path.abspath(\n",
    "        r\"S:\\Data\\Stores Payroll\\FY21\\99_Master Scripts (DO NOT EDIT)\\common_functions\"))\n",
    "\n",
    "from halfords_functions import newest, halfords_week\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Credits FY21 \n",
    "\n",
    "* This script maniuplates course_credits and adds them into SQL.\n",
    "\n",
    "* We first merge all the files into one \n",
    "\n",
    "* we then drop duplicates based on certain criterion. \n",
    "\n",
    "* we allocate all the correct hours to each course.\n",
    "\n",
    "* we save a file for Jonty for FY21.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_credits_raw = r'S:\\Data\\Stores Payroll\\FY21\\02_Weekly Tasks\\Extra Hours\\raw_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fy21 dates.\n",
    "\n",
    "dates = pd.read_sql(\"SELECT * from fy21_calendar\", con=engine)\n",
    "\n",
    "# structure tab wtih shop names etc.\n",
    "\n",
    "structure = pd.read_sql(\"SELECT Shop as store, Location, Area, Division from structure_tab\", engine)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As today is 2019-09-26 we haven't started FY21 so we will be using a psuedo week number which is calculated\n",
      "from the distance of weeks from FY21\n",
      "We are -27 weeks away from FY21\n"
     ]
    }
   ],
   "source": [
    "file_name,week_,day_ = halfords_week(dates)\n",
    "print(f\"We are {week_} weeks away from FY21\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self defined function to grab multile file types.\n",
    "\n",
    "\n",
    "def get_files(extensions,path_to_search):\n",
    "    all_files = []\n",
    "    for ext in extensions:\n",
    "        all_files.extend(Path(path_to_search).glob(ext))\n",
    "    return all_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets pass all the files into a list \n",
    "\n",
    "training_files = get_files(['*.csv','*.xlsx'],training_credits_raw)\n",
    "\n",
    "\n",
    "# lets split out the wsadmin report and remove it from the main list as the size of the data is different.\n",
    "\n",
    "for file in training_files:\n",
    "    if 'wsadmin' in str(file):\n",
    "        appren_file = file\n",
    "        training_files.remove(file)\n",
    "        \n",
    "dfs = []\n",
    "for file in training_files:\n",
    "    n = str(file).split('.')[-1]\n",
    "    if n == 'csv':\n",
    "        dfs.append(pd.read_csv(file))\n",
    "    else:\n",
    "        dfs.append(pd.read_excel(file))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Training Courses (non apprentice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_courses = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse column names to lower case and replace spaces with _\n",
    "\n",
    "training_courses.columns = [cols.lower().replace(' ','_') for cols in training_courses.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove cancelations - we don't credit or care about these.\n",
    "\n",
    "training_courses = training_courses.loc[training_courses['status'] != 'User Cancelled']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "## lets remove duplicates from non-aspire courses. \n",
    "\n",
    "## The logic is to remove duplicates if they aren't booked or fully attended. \n",
    "## People usually double book after they were accepted - we only want to credit those have have been booked - ie.\n",
    "## approved by their store managers and remove those are 'pending' to be approved.\n",
    "\n",
    "non_aspire = training_courses.loc[\n",
    "    (training_courses[\"course_name\"].str.contains(\"Aspire\") == False)\n",
    "    & (\n",
    "        ~training_courses.duplicated([\"username\", \"course_name\"], keep=False)\n",
    "        | training_courses['status'].ne(\"Booked\", \"Fully Attended\")\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split out aspire so we can re-merge these into one file.\n",
    "\n",
    "aspire = training_courses.loc[\n",
    "    (training_courses[\"course_name\"].str.contains(\"Aspire\") == True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_cleaned = pd.concat([aspire,non_aspire])\n",
    "\n",
    "training_cleaned['date'] = pd.to_datetime(training_cleaned['session_start_date'],dayfirst=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_keep = ['username',\n",
    " \"user's_fullname\",\n",
    " \"user's_organisation_name\",\n",
    " 'course_name',\n",
    " 'date',\n",
    " 'status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove columns that we don't need.\n",
    "\n",
    "training_cleaned = training_cleaned[cols_to_keep].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "## adding one year to ensure cal works for next year. \n",
    "\n",
    "training_cleaned['date'] = training_cleaned['date'] +  pd.Timedelta(days=365)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handle Missing Courses from the raw_reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "course_mapper_updated\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "os.chdir(r'S:\\Data\\Stores Payroll\\FY21\\02_Weekly Tasks\\Extra Hours\\course_mapper')\n",
    "\n",
    "current_course_names = training_cleaned[['course_name']].drop_duplicates()\n",
    "\n",
    "c_mapper = pd.read_excel(r\"S:\\Data\\Stores Payroll\\FY21\\02_Weekly Tasks\\Extra Hours\\course_mapper\\course_mapper_master.xlsx\")\n",
    "\n",
    "checker = c_mapper['course_name'].tolist()\n",
    "\n",
    "print('course_mapper_updated')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_courses = current_course_names.loc[\n",
    "    ~current_course_names[\"course_name\"].isin(checker)\n",
    "][\"course_name\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Touring Workshop 2018 is missing from the course_mapper_master_file please add it\n",
      "Touring Workshop 2018 is missing from the course_mapper_master_file please add it\n",
      "Touring Workshop 2018 is missing from the course_mapper_master_file please add it\n",
      "Touring Workshop 2018 is missing from the course_mapper_master_file please add it\n",
      "Touring Workshop 2018 is missing from the course_mapper_master_file please add it\n",
      "The missing course names have been saved to the course_mapper_folder - please use the exact name with -\n",
      "no leading or trailing spaces in the master_file\n"
     ]
    }
   ],
   "source": [
    "if len(missing_courses) > 0:\n",
    "    pd.DataFrame({\"Missing Courses\": missing_courses}).to_excel(\n",
    "        \"Missing_Courses.xlsx\", index=False\n",
    "    )\n",
    "    for name in missing_courses:\n",
    "        print(f\"{name} is missing from the course_mapper_master_file please add it\")\n",
    "    print(\n",
    "        \"\"\"The missing course names have been saved to the course_mapper_folder - please use the exact name with -\\nno leading or trailing spaces in the master_file\"\"\"\n",
    "    )\n",
    "else:\n",
    "    print(\"All reports are present in the report\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finish cleaning the raw_reports and add in the correct hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_cleaned = pd.merge(training_cleaned,dates,on='date',how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_cleaned['store'] = training_cleaned['user\\'s_organisation_name'].str.extract('(\\d+)').fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_cleaned.drop('user\\'s_organisation_name',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_cleaned = pd.merge(training_cleaned,c_mapper,on='course_name',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_cleaned.loc[training_cleaned['status'].isin(['No show','Declined','Requested']),'hours'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create the reason column - this is for the SM's information and clarity.\n",
    "\n",
    "training_cleaned['reason'] = (\n",
    "    training_cleaned[\"course_name\"]\n",
    "    + \": \"\n",
    "    + training_cleaned[\"user's_fullname\"]\n",
    "    + \" \\\\\\\\ \"\n",
    "    + training_cleaned[\"status\"]\n",
    "    + ':'\n",
    "    + training_cleaned['date'].dt.strftime(\"%a %b %y\") \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_cleaned.rename(columns={'course_name' : 'type'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_final = training_cleaned[['username','type','store','retail_ops_week','reason','hours']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apprenticeships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same logic as above, but calcs are done by start + end time, need to use some handy regex (regular expressions) for this.\n",
    "\n",
    "appr = pd.read_excel(appren_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "appr.columns = [cols.lower().replace(' ','_') for cols in appr.columns]\n",
    "\n",
    "appr['store'] = appr['user\\'s_organisation_name'].str.extract('(\\d+)').fillna(0).astype(int)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "appr['date'] = pd.to_datetime(appr['session_start_date'],dayfirst=True)\n",
    "\n",
    "appr['date'] = appr['date'] +  pd.Timedelta(days=365)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "appr = appr.loc[appr['status'] != 'User Cancelled'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract start + end times using regex to calculate the differences as python datetime objects. \n",
    "\n",
    "a = pd.to_timedelta(\n",
    "    pd.to_datetime(\n",
    "        appr[\"session_start_time\"].str.extract(\n",
    "            r\"\\b((1[0-2]|0?[1-9]):([0-5][0-9]) ([AaPp][Mm]))\", expand=False\n",
    "        )[0]\n",
    "    ).dt.strftime(\"%H:%M:%S\")\n",
    ")\n",
    "\n",
    "b = pd.to_timedelta(\n",
    "    pd.to_datetime(\n",
    "        appr[\"session_finish_time\"].str.extract(\n",
    "            r\"\\b((1[0-2]|0?[1-9]):([0-5][0-9]) ([AaPp][Mm]))\", expand=False\n",
    "        )[0]\n",
    "    ).dt.strftime(\"%H:%M:%S\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "# round up to 15min intervals and calc hours.\n",
    "#b = end time, a = start time\n",
    "\n",
    "hours = (b-a).dt.round(freq='15min').dt.seconds / 3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "appr['hours'] = hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "appr.loc[appr['status'].isin(['Requested','No show']),'hours'] = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "appr['reason'] = (\n",
    "    \"Apprenticeship \"\n",
    "    + appr[\"user's_fullname\"]\n",
    "    + \": \"\n",
    "    + appr[\"status\"]\n",
    "    + ' \\\\\\\\'\n",
    "    + appr[\"date\"].dt.strftime(\"%a %b %y\")\n",
    "    + '- '\n",
    "    + pd.to_datetime(pd.to_timedelta(a)).dt.strftime(\"%X\")\n",
    "    + ' : '\n",
    "    + pd.to_datetime(pd.to_timedelta(b)).dt.strftime(\"%X\")\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "appr['type'] = 'apprenticeships'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "appr = pd.merge(appr,dates,on='date',how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current breakdown of hours by status\n",
      "status\n",
      "Booked                 1097.25\n",
      "Fully attended        10248.50\n",
      "No show                   0.00\n",
      "Partially attended       59.50\n",
      "Name: hours, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "final_appren = appr[['username','type','store','retail_ops_week','reason','hours']]\n",
    "\n",
    "print(\"The current breakdown of hours by status:\")\n",
    "print(appr.groupby('status')['hours'].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concat both files and add in columns for database.\n",
    "\n",
    "* final bit of prep before saving file down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([final_appren,training_final])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['store'] = df['store'].fillna(0).astype(int).astype(str).str.zfill(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['shop'] = df['store'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['CostCentre'] = 5001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Week Number'] = df['retail_ops_week'] - 2100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"BusinessFunction\"] = \"Hub Team\"\n",
    "df[\"Owner\"] = \"Hub Team\"\n",
    "df[\"Rate\"] = 9\n",
    "df.rename(columns={\"username\": \"EmployeeNumber\"}, inplace=True)\n",
    "\n",
    "df = df[\n",
    "    [\n",
    "        \"store\",\n",
    "        \"shop\",\n",
    "        'retail_ops_week',\n",
    "        \"hours\",\n",
    "        \"reason\",\n",
    "        \"CostCentre\",\n",
    "        \"type\",\n",
    "        \"Rate\",\n",
    "        \"Owner\",\n",
    "        \"BusinessFunction\",\n",
    "        \"EmployeeNumber\",\n",
    "        \"Week Number\",\n",
    "    ]\n",
    "].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = pd.read_sql(\"SELECT TOP 1 * From extrahoursdetails\",engine).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r'S:\\Data\\Stores Payroll\\FY21\\02_Weekly Tasks\\Extra Hours\\outputs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('extra_hours_' + file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_files = get_files(['*.csv','*.xlsx'],training_credits_raw)\n",
    "\n",
    "for file in training_files:\n",
    "    file.rename(Path(file.parent, f\"{file_name}_{file.stem}{file.suffix}\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_files = get_files(['*.csv','*.xlsx'],training_credits_raw)\n",
    "\n",
    "for file in training_files:\n",
    "    shutil.move(str(file), os.path.join(str(file.parent) + '\\\\processed', str(file).split('\\\\')[-1]))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
